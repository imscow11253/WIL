
25.2.3
58p ~ 79p

## Chapter 2 마르코프 결정 과정 (MDP)
---
밴디트 문제는 에이전트가 어떤 행동을 취하든지에 상관 없이 문제의 설정은 변하지 않는다. 비정상 문제는?

> [!note] 비정상 문제는 시간에 따라서만 확률이 변하는 것이다. 여기서는 에이전트의 행동에 따라 확률이 변하는 것을 배워볼 것이다.

마르코프 결정 과정에서는 ==타임== 개념이 중요하다. 타임에 따라 에이전트 행동이 달라지고, 그 행동에 따라 문제의 확률이 달라지기 때문이다. 


## MDP 란?

마르코프 결정 과정은 에이전트의 행동에 따라 문제의 확률이 변경되는 문제상황에서, ==에이전트와 환경의 상호작용을 수식으로 표현한 것이다.== 

이 상황을 그림으로 표현하면 다음과 같다. 
![[Pasted image 20250203200422.png]]

특정 시점 t 에서 에이전트가 S(t)의 상태에 놓여있었다. 거기에서 행동 A(t)를 선택하면 보상 R(t)를 받고, 상태는 S(t+1)이 되는 형태인 것이다. 

뭔가 주체와 각 주체가 결정하는 데이터를 나타내면 다음과 같다.
- 에이전트
	- 정책 (에이전트가 행동을 결정하는 방법)
- 환경
	- 상태 전이 (행동에 따라 변화하는 환경)
	- 보상 (행동에 따른 보상)


## 각 데이터를 수식으로 표현해보자

MDP는 위 그림 환경을 수식으로 나타낸 것이므로, 각 주체에서 결정하는 데이터를 수식으로 표현해보자. 

### 1. 상태전이
우선 상태 전이부터 해보자.

상태 전이는 ==결정적==으로 이루어질 수도 있고 ==확률적==으로 이루어질 수도 있다. 즉 에이전트가 특정 행동 a를 하더라도 상태가 b로 결정하는 것이 결정적이라는 것이고, b가 될수도, c가 될수도 있는 것이 확률적이라는 것이다. 

결정적 
![[Pasted image 20250203201122.png]]
s 상태에서 a 행동을 했을 때 s' 상태로 전이 된다는 뜻의 수식이다. 

확률적
![[Pasted image 20250203201132.png]]
s 상태에서 a 행동을 했을 때, s' 가 될 확률을 나타내는 수식이다. 


### 2. 보상
보상은 쉽다. 

![[Pasted image 20250203201339.png]]

상태 s 에서 a 행동을 선택했을 때 s' 상태로 전이되면 나오는 보상 값을 위와 같이 표현한다. 


### 3. 정책
에이전트가 특정 행동을 결정하는 정책도 상태와 마찬가지로 결정적, 확률적으로 나눌 수 있다. 

결정적 
![[Pasted image 20250203201503.png]]
s 상태에서 어떤 행동을 할 것이가를 나타내는 함수를 뮤라고 표현한다. 여기서는 s 상태를 뮤 정책에 넣었을 때 a 행동을 도출한 것을 나타낸 수식이다. 

확률적
![[Pasted image 20250203201555.png]]
s 상태에서 a 행동을 선택할 확률을 나타내는 수식이다. 


이렇게 문제상황 속 각 주체가 다루는 데이터를 수식으로 표현했다. 


## MDP의 목표 : 최적 정책 찾기

보상이 최대가 되는 정책을 찾는 것이 MDP의 목표이다. 

### 일회성/지속적 과제

우선 일회성 과제, 지속적 과제가 뭔지부터 알아야 한다. MDP는 문제에 따라 일회성/지속적 과제로 나뉜다. 
일회성 과제는 끝이 있는 문제를 나타내고, 지속적 과제는 끝이 없는 문제를 나타낸다. 

바둑 같은 일대일 게임은 한 사람이 승리하면 끝나기 때문에 일회성 과제이고, 재고 관리 시스템 같은 경우는 재고 창고가 존재하는 한 계속 끝없이 이어져야 하므로 끝이 없다. 즉 지속적 과제이다. 

### 수익

수익이란 특정 상태에 존재할 때 앞으로 얻을 수 있는 보상들의 총 합을 나타낸다. 즉 MDP 목표는 수익을 극대화 하는 것이라고 할 수 있다. 

![[Pasted image 20250203202625.png]]

이렇게 나타낼 수 있다. 그림이 좀 깨졌는데 G(t) = R(t) + rR(t+1) + (r^2)R(t+2) ... 이다. 미래에 얻을 수 있는 보상을 전부 합한 것이다. 근데 r은 뭐냐? ==할인율==이다.
원래는 그냥 보상들을 더할 것이다. (= r의 값이 1일 것이다.) 이러면 문제점이 수익이 무한대가 된다는 것이다. 이를 방지하기 위해 할인율을 0과 1 사이의 값으로 두고 곱해서 미래의 보상일수록 점점 작아지게 만드는 것이다. 

또 하나 좋은 점은 가까운 미래의 보상일수록 수익에 크게 반영되도록 한다는 점이다. 

### 근데 정책, 상태변이 등이 확률적으로 존재할 수 있잖아?

그래서 수익 수식을 그대로 사용하는 것이 아니라 ==수익의 기댓값==을 지표로 삼아야 한다. 
![[Pasted image 20250203203219.png]]

여기서 좌변의 v(파이) = E[G(t) | S(t), 파이] 가 ==상태 가치 함수== 이다. 즉 (수익의 기댓값 = 상태 가치 함수) 이다. 즉 MDP의 목표는 상태가치함수를 최대화하는 것이라는 것이다.


## MDP 예제

정책과 상태전이가 결정적이라고 가정
![[Pasted image 20250203204039.png]]

![[Pasted image 20250203204105.png]]

여기서 가장 값이 높은 정책은 뮤2이다. 계산해보면 v(뮤2)(s) 가 가장 큰 값을 가진다. 














