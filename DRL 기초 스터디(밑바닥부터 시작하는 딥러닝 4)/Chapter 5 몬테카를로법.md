

25.2.12
149p ~ 188p

# Chapter 5 몬테카를로법
---

DP를 사용하면 연립 방정식의 연산 지옥에서 나올 수 있다. 하지만 DP를 사용하기 위해서는 상태 전이 함수, 보상 함수를 알고 있어야 한다. 현실의 문제는 알 수 없는 경우가 대다수이다. 그리고 그리드 문제 같은 경우에는 에이전트의 환경에 따라 상태 전이 함수, 보상 함수가 달라지므로 비정상 문제라고 할 수 있다.

그럼 Chapter 1 의 밴디트 문제처럼 직접 데이터를 수집하면서 정책을 개선하는 수 밖에 없다. 탐색-활용의 trade-off 정도를 잘 조절 해야 한다. 

### 몬테카를로법

몬테카를로법은 별거 없다. 그냥 직접 수행해보면서 기댓값(= 상태 가치 함수)를 수정해가는 방식이다. 실제 분포 모델을 알 수 없으니까 샘플링 하면서 기댓값을 수정해가는 것이다. 수학에서 배운 큰 수의 법칙에 따르면 샘플링 수가 늘어날 수록 분포모델의 확률과 같아진다. 그치만 샘플링을 많이 한다는 것 자체가 ==탐색==을 많이 해본다는 것인데 그럼 에이전트가 최적의 선택만을 할 순 없다는 뜻이다. 

![[Pasted image 20250213164922.png]]

### 상태 가치 함수를 몬테카를로법으로 구해보자.

상태 s에서 샘플링해서 얻은 수익을 G라고 표현하면 상태 가치 함수를 다음과 같이 표현할 수 있다. 
![[Pasted image 20250213165303.png]]

즉 특정 상태 s에서 특정 정책에 대해서 상태 가치 함수를 몬테카를로법으로 구하고 싶으면, 상태 s에서 특정 정책에 따라 여러 번 시도해보고 나온 수익들의 기댓값을 구하면 되는 것이다. 
![[Pasted image 20250213165409.png]]

근데 상태가 무척 많으면 계산해야 하는 연산량이 무척 많을 것이다. (상태의 수) X (샘플링 수) 일 것이다. 

![[Pasted image 20250213165501.png]]

그래서 최적화를 하면 좋다. 만약 상태 A에서 출발했더라도 중간에 B,C를 거칠 수 있다. 그렇게 중복되는 샘플링을 모으면, 중복되는 연산을 줄일 수 있을 것이다. 

![[Pasted image 20250213165609.png]]
![[Pasted image 20250213165627.png]]

수식으로 표현하면 다음과 같다. 
![[Pasted image 20250213165636.png]]

그리고 파이썬 코드로 구현한 부분이 책 163p에 있다. 

### 정책 평가는 했는데, 정책 개선은 어떻게?

기본적인 개념은 DP에서 했던 정책 반복법과 같다. 정책 평가로 상태 가치 함수를 도출하고, 도출한 결과 값으로 정책을 수정한다. 
![[Pasted image 20250213170136.png]]

다만 이걸 그대로 사용하면 문제가 2가지 존재한다. 2가지 다 Chapter 1 밴디트 문제에서 겪었던 문제다. 

1. Greedy 알고리즘으로 되기 때문에 탐색-활용 중 활용만 된다.
2. 에이전트의 상태에 따라 상태전이함수/보상함수가 변경되는 비정상 문제인데 그냥 기댓값 계산하면 최신의 보상이랑 예전 보상의 가치가 같아진다. 

각각 해결법은 밴디트 문제에서의 해결법과 동일하다. 

1. 입실론-Greedy 정책을 도입해서 일정 비율로는 무작위 행동을 하도록 함.
2. 기댓값을 구할 때 1/n을 곱하는 것이 아니라 0과 1 사이의 값인 할인율을 곱해줌.

만약 행동 가치 함수를 몬테카를로로 구하면 행동 가치 함수 값과 정책 값이 각각 다음 도식과 같이 표현될 수 있다. 

![[Pasted image 20250213170457.png]]

![[Pasted image 20250213170644.png]]


### 온-오프 정책은 한 걸음 더 나아가기다. 부록 A 참조

탐색-활용으로 인해 에이전트는 최적의 선택을 할 수 없게 되었다. 그럼 탐색과 활용을 분리해서 탐색을 하는 정책과 활용을 하는 정책을 분리해서 탐색하는 데이터를 활용하는 정책에 적용하면 어떨까 라는 개념이다. 
책을 읽어보자. 