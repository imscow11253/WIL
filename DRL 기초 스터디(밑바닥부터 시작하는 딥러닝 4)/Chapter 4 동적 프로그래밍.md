
25.2.10
107p ~ 148p

# Chapter 4 : 동적 프로그래밍
---

### remind

정책, 상태전이, 보상이 존재하는 문제에서 에이전트가 취해야 할 행동을 정의하기 위해서 최적의 정책을 찾아야만 했고, 최적의 정책을 찾기 위해서 정책 간 평가를 할 수 있는 척도가 필요했다. 그게 상태 가치 함수/행동 가치 함수 였다. 그런데 해당 척도에는 무한의 개념이 들어간 '수익'이 있기 때문에 사실상 계산이 불가했다. 
이 무한의 개념을 없애고 평가 척도 함수를 계산할 수 있게 만들어준 것이 바로 Chapter 3에서 배운 ==벨만 방정식==이다. 

벨만 방정식은 무한의 개념이 포함된 수익을 소거하는 연립 방정식을 사용해서 상태 가치 함수의 값을 구한다. 하지만 현실의 문제는 녹록치 않다. 에이전트가 수행할 수 있는 행동이 수만가지고 그 행동을 결정하는 정책은 또 얼마나 많을까...!! 정책 뿐만 아니라 환경의 '상태'가 많아져도 마찬가지이다. 이런 상태에서 모든 정책에 대해서 연립 방정식을 하면 몇 차 방정식일지 상상도 안가고, 계산량이 어마어마할 것이다.

그래서 나온 것이 동적 프로그래밍이다. 

### 시작하기 전에...

다시 읽으면서 상태 가치 함수/행동 가치 함수에 대한 의미론적인 이해가 부족하다는 것을 느꼈다. 다시 한 번 더 짚고 넘어가자.

==상태 가치 함수== : 각 상태에서 특정 정책이 가지는 가치 판단 값
즉, 3x3 그리드 세상에서 특정 정책(ex - 각 칸(=상태)에서 취할 수 있는 모든 행동이 동일한 확률을 가지는 정책)의 상태 가치 함수를 계산한다고 하면 총 9개의 상태 가치 함수 값이 나온다. (= 각 칸에서 해당 정책의 가치 값)

==행동 가치 함수== : 각 상태에서 특정 행동이 가지는 가치 판단 값
상태 가치 함수에서는 정책에 대한 가치 판단 값이라면, 행동 가치 함수에서는 행동에 대한 가치 판단 값이다. 3x3 세상에서, 그리고 각 칸에서 취할 수 있는 행동이 2개씩 이라면 9x2 =18 개의 행동 가치 값이 나오게 된다. 

> [!note] 수식을 이해하는 것 뿐만 아니라, 해당 수식의 의미론적인 이해도 같이 하자.

### 동적 프로그래밍

벨만 방정식을 사용해서 각 상태에서 상태 가치 함수 값을 구하는 것은 연립방정식을 사용해서 도출할 수 있었다. 근데 그건 굉장히 많은 연산량의 요구한다 그래서 나온 것이 동적 프로그래밍, DP이다. 

DP의 개념은 '했던 계산을 또 진행하지 않는 다'는 점에서 알고리즘에서 사용하는 DP랑 다를 것이 없다. DP에서 Bottom-Up 방식으로 값을 계산해 나갈 때, 기저 값과 점화식을 정의하고 사용하는데, 벨만 방정식으로 상태 가치 함수를 구할 때 같은 방식을 사용하므로, 이번 chapter 이름이 DP인 것이다. 

### 벨만 방정식을 DP 점화식로 나타내보자.

![[Pasted image 20250212142854.png]]

이게 벨만 방정식 식이었다. 다시 한 번 더 의미를 설명해보자면, 현재 상태의 상태 가치 함수를 현재 상태에서 취할 수 있는 행동으로 인해 발생될 수 있는 다음 상태들의 상태 가치 함수들로 표현한 식이다. 

이 식을 점화식으로 사용할 것이다. 책에서는 갱신 식이라고 표현한다. 
![[Pasted image 20250212143110.png]]

식을 설명해보자면 K 가 현재 시간에서의 값을 나타내고, K+1 은 단위시간 뒤에 갱신된 값을 의미한다. 의미론적으로 설명해보면, s에서의 상태 가치 함수를 구하기 위해서 s에서 취할 수 있는 행동들로 인해 전이될 수 있는 상태들의 상태 가치 함수 값을 이용하겠다는 것이다. 

여기서 각 상태에서 상태 전이 함수를 0으로 전부 설정해두고,  정책 함수/상태전이 함수/보상 함수 의 값을 알고 있다면 각 상태에서의 상태 전이 함수를 계속해서 무한히 갱신해 갈 수 있다. 

> [!note] 이렇게 스스로 자신의 값을 갱신해간다는 과정을 부트스트래핑 (bootstrapping) 이라고 한다.

오케이. 이 점화식을 사용하면 상태 전이 함수 값이 갱신된다는 것은 알겠음.

### 근데 값이 발산할 수도 있는거고, 수렴 되더라도 그게 원하는 값이 맞는지 확신은 어떻게 함?

이건 이미 증명되어 있다. 몇 가지 조건만 만족하면, 갱신되는 값이 결국에 실제로 해당 상태에서 특정 정책에 대한 상태 가치 함수 값으로 수렴한다. 자세한 내용은 109P에 reference로 주어지는 다른 문헌을 참고 하자.

그리고 책에서는 2칸 짜리 그리드 월드와 3x4 짜리 그리드 월드에서 실제로 파이썬 코드를 작성해가면서 설명을 한다. 110P ~ 128P


이렇게 DP로 상태 가치 함수를 계산하는 방법을 ==반복적 정책 평가== 라고 한다. 

### 정책 반복법

자 그럼, 특정 정책에 대해서 각 상태에서 가치를 판단하는 방법을 알았다. (벨만 방정식에도 알았지만, 연산량이 적은 방법으로 개선된 방법을 알게 되었다.)

근데, 우리가 관심 있는 것은 최적의 정책을 찾는 것이다. 이때 사용할 수 있는 방법이 ==정책 반복법==과 ==가치 반복법==이다. 먼저 정책 반복법부터 알아보도록 하자. 

3장에서 배웠던 최적 정책은 다음과 같이 표현할 수 있다. 
![[Pasted image 20250212144453.png]]

의미론적으로 설명을 해보면, 현재 내 상태 s에서 취할 수 있는 모든 행동에 대해서 행동 가치 함수를 비교하고, 행동 가치 함수 값이 가장 크게 될 수 있는 행동 a를 선택하는 것이다. 

이걸 앞에서 했던 것처럼 점화식으로 표현해보자. 

![[Pasted image 20250212144607.png]]

이렇게 표현을 하게 되면, 내 상태 s의 다음으로 될 수 있는 모든 상태 s' 의 각각 상태 가치 함수값을 알고 있다면 현재 내가 취할 행동을 결정할 수 있다는 것이다. 

근데 그 행동이 최적 결정이라는 것은 확신할 수 없다. 왜냐? 일단 상태 가치 함수값도 최적 정책으로부터 도출된 것이 아니라 특정 시작 정책으로부터 계산된 값이기 때문이다. 이렇게 정책을 계속 업데이트 해가는 것이다. 
이렇게 업데이트 해나가면 결국 최적 정책으로 수렴하는데, 이에 대한 증명은 이미 되어있다. 이 또한 책에서 reference로만 언급한다. 관심있으면 찾아보자. (131P 정책 개선 정리)

### 상태 가치 함수 점화식, 정책 점화식 도출했으니 같이 써보자. (=정책 반복법)

자, 그럼 이제 상태 가치 함수 값으로부터 정책을 개선해나갈 수 있다. 그럼 해보자. 반복적 정책 평가에 의해, 특정 정책을 임의로 정하고, 각 상태에 대한 상태 가치 함수 값을 0으로 모두 설정해두고 점화식을 돌리면 처음에 정한 특정 정책에 대한 모든 상태에 상태 가치 함수 값을 도출할 수 있다. 

모든 상태 가치 함수 값을 도출했으니, 두 번째로 도출한 점화식에 의해서 정책을 개선시킬 수 있다.  (= 각 상태에서 정책 함수 값을 개선할 수 있다.) 또 정책이 개선되었으니, 다시 상태 가치 함수 값을 다시 0으로 시작해서 반복적 정책 평가로 새롭게 개선된 정책에 대한 상태 가치 함수 값을 도출할 수 있다. 

이렇게 정책을 평가하고 (=반복적 정책 평가) 정책을 개선하는 방법을 번갈아서 진행하면, 정책이 변화하지 않는 순간이 올텐데 그게 최적의 정책이다. (이게 증명되어 있음.)

이렇게 번갈아가면서 정책을 개선해 가는 방법을 ==정책 반복법==이라고 한다. 

그림으로 표현하면 다음과 같다. 
![[Pasted image 20250212145421.png]]

책 132P에 3x4 그리드 월드에서의 파이썬 코드 예제가 있다. 


### 가치 반복법

![[Pasted image 20250212145600.png]]

앞선 정책 반복법을 그림으로 포현하면 다음과 같다. 원래는 다차원적이고, 복잡하지만 단순화해서 2차원으로 표현한 것이다. 

근데 다음과 같이 표현하면 연산량이 적을 수 있다. 
![[Pasted image 20250212145749.png]]

각 그림이 의미하는 바가 뭘까?
첫 번째 그림에서 파란 화살표는 반복적 정책 평가 방법에 의해 모든 상태 가치 함수 값이 갱신되는 것을 의미한다. 다음으로 주황 화살표는 상태 가치 함수 값으로 정책을 개선하는 것을 의미한다. 

첫 번째 그림에서 경계선까지 화살표가 이어져있는 것의 의미는 모든 상태 가치 함수를 전부 수렴할 때까지 갱신한다는 것이고, 또 모든 상태에 대해서 정책을 개선한다는 뜻이다. 

그럼 반대로 두 번째 그림의 의미를 해석해 볼 수 있을텐데, 의미는 모든 상태에 대해서 계산을 전부 완전히 계산하는 것이 아니라, 일부분까지만 수행하고 다음 단계로 넘어간다는 뜻이다. 그래도 특정 값으로 수렴한다는 것이고, 오히려 연산량이 적게 수행된다는 것이다. --> 이해는 되는데 증명은 모르겠음. 책에서도 안나옴.

위의 첫 번째 그림과 두 번째 그림을 책의 예시인 그리드 월드로 표현하면 각각 다음 그림과 같다. 

![[Pasted image 20250212150254.png]]

![[Pasted image 20250212150301.png]]

각 그림의 의미를 다시 설명해보자. 

첫 번째 그림은 특정 정책에 대해서 모든 상태 (=모든 그리드 칸)에서 상태 가치 함수 값을 계산한다. 그것도 수렴할 때까지... 그리고 수렴이 되거나 임계값을 만족하면, 도출한 상태 가치 함수를 가지고 모든 상태 (=모든 그리드 칸)에서의 정책을 개선한다는 의미이다. 

두 번째 그림은 모든 상태 (=모든 그리드 칸)에 대해서 연산을 하겠다는 것이 아니라 상태 하나씩 평가-개선을 하겠다는 것이다. 

![[Pasted image 20250212150543.png]]

개선과 평가를 각각 수식으로 표현하면 다음과 같다. 

근데 같은 계산이 있으므로 묶을 수가 있다. 묶으면 다음과 같이 표현된다. 
![[Pasted image 20250212151300.png]]

즉, 정책의 개선 식 없이 상태 가치 함수만 갱신하는 방식으로 최적 정책에 대한 상태 가치 함수를 구할 수 있다는 것이다. 

그렇게 최적 정책에 대한 모든 상태에서 상태 가치 함수 값이 주어지면 최적 정책은 다음식으로 구할 수 있다. 
![[Pasted image 20250212151430.png]]

그리고 144P에 파이썬 예시가 있다.

### 정리
DP를 이용해서 최적 정책을 구하는 방법을 배웠다. 크게 ==정책 반복법==과 ==가치 반복법==이 있다. 정책 반복법은 정책의 평가와 개선을 번갈아가면서 수행하는데 평가를 하기 위해서 ==반복적 정책 평가=== 방법을 사용한다. 
가치 반복법은 평가와 개선을 융합한 방법이다. 정책을 개선할 필요 없이 상태 가치 함수 값을 갱신하는 것만으로 최적 정책에 대한 상태 가치 함수 값으로 수렴 시킬 수 있다. 그렇게 도출한 상태 가치 함수로부터 최적 정책을 구할 수 있다. 

DP 개념은 반복적 정책 평가 방법, 정책 개선식, 정책 반복법/가치 반복법 전반에 걸쳐서 적용되고 있다. 



