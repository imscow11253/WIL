
25.2.3
80p ~ 106p

# Chapter 3 : 벨만 방정식
---

### remind

2장에서 마르코프 결정 과정 (MDP)를 배웠다. 강화학습에서 마주하는 문제를 수식으로 표현했다. 간단하게 개념만 정리해보자. 

요소를 크게 3가지로 나눌 수 있다. 
1. 정책 (에이전트가 행동을 결정하는 방식)
2. 상태 전이 (에이전트가 특정 행동을 했을 때 변하는 환경의 상태)
3. 보상 (에이전트가 특정 행동을 해서 변하는 환경에 따른 리워드)

여기서 에이전트가 특정 상태 s 일 때, 가장 최적의 정책을 찾기 위한 평가 척도가 ==상태 가치 함수== 이고, 이걸 식으로 나타냈었다. 그리고 정책과 상태 전이는 결정적일수도, 비결정적일 수도 있기 때문에 확률적으로 표현해야 하므로, 상태 가치 함수가 기댓값의 형태로 표현되었다. 

자, 그럼 에이전트가 특정 상태 s 에 있을 때 어떤 정책을 선택할지 결정하기 위해서 정책들의 후보들을 추린 후에 각 후보 정책의 상태 가치 함수를 계산하고 비교해서 최적의 정책을 선택하면 된다.

이렇게 결론내면 참 좋겠지만 상태 가치 함수를 보면 G (수익) 이라는 무한의 개념이 포함된 값이 등장한다. 
![[Pasted image 20250203203219.png]]
![[Pasted image 20250203202625.png]]

G (수익)은 미래에 얻을 수 있는 보상에다가 할인율을 곱해서 더한 값으로, 미래가 무한하기 때문에 수익을 구할 때 무한의 개념이 들어가는 것이다. 

그래서 상태 가치 함수의 값을 얻어내기가 힘든데, 이때 우리를 구제해주는 것이 ==벨만 방정식==이다. 


### 벨만 방정식

벨만 방정식을 도출해보자. 유도 과정이 왜 그런 것이냐 라는 질문에는 의문을 가지지 말자. 이미 벨만이라는 사람이 개천재라서 그렇게 도출한 것 뿐이니...

![[Pasted image 20250203202625.png]]

위의 식은 수익의 식이다. 여기서 t 에 t+1 을 대입해본다. (= 시간이 단위 시간만큼 흘렀다.)

![[Pasted image 20250207141652.png]]
(pdf 자료가 화질이 구려서 +가 -로 보이거나 . 으로 보일 수 있는데 +이다. )

이걸 기존의 수익 식에 대입하면 G(t) 와 G(t+1)의 관계식을 도출할 수 있다. 
![[Pasted image 20250207141803.png]]


이렇게 G(t) 와 G(t+1)의 관계를 알았으니 이걸 사용해서 상태 가치 함수를 표현하면 다음과 같다. 
![[Pasted image 20250207141849.png]]

마지막 식의 전개는 기댓값의 '선형성' 때문이다. 
즉 상태 가치 함수를 알기 위해서는 R(t) | s 와 G(t+1) | s 의 기댓값을 구할 수 있으면 된다. 
사실상 해당 값을 구하는 건 또 무한의 개념이 들어가는 거라 안되고, 이 식을 적절하게 변형해서 t에서의 상태 가치 함수와 t+1 에서의 상태 가치 함수의 관계식을 나타내 볼 것이다. 

![[Pasted image 20250207142200.png]]
우선 앞의 식인 이걸 먼저 구해보자. 

![[Pasted image 20250207142138.png]]

이 그림을 잘 보면서 해당 식이 어떤 의미를 가지는지 알면 식을 전개하기가 편하다. 
저 식의 의미는 ==현재 에이전트가 상태 s에 있을 때 정책 π 를 따르면 얻을 수 있는 보상의 기댓값==이다. 기댓값이니까 또 풀어서 쓰면 ==현재 에이전트가 상태 s 에 있을 때, 정책 π 를 따른 모든 경우의 수를 다 해보고 얻은 보상들의 평균==이다. 

이를 풀어서 쓰면 다음과 같이 작성할 수 있다. 
![[Pasted image 20250207142447.png]]

π, p, r 은 각각  정책 함수, 상태전이 함수, 보상 함수를 나타내고, 이는 2장에서 배웠다. 
풀어서 해석하면 현재 s 상태에 있을 때, 정책 π 에 따르는 모든 행동을 해보고 얻는 보상들의 총합 이라고 할 수 있다. 


그럼 상태 가치 함수의 첫 번째 식은 표현을 끝냈고, 두 번째 식을 전개해보자. 
![[Pasted image 20250207142658.png]]


감마는 일단 할인율을 나타내므로 개발자가 결정하는 상수라 일단 무시한다. 
위의 식의 의미는 ==현재 에이전트가 상태 s에 있을 때, 정책 π 를 따르면 얻을 수 있는 t+1의 보상의 기댓값==이다. 다음 그림을 보면 이해가 좀 될 수 있을 것 같다. 

![[Pasted image 20250207143039.png]]


즉 현재 내 상태 s 에서 정책 π 를 따라서 모든 갈 수 있는 모든 상태 s' 에 간 후에 각 s' 에서 정책 π에 따른 상태 가치 함수를 모두 구하고, 그거의 기댓값을 나타낸다는 것이다. 이걸 식으로 표현하면 다음과 같다. 

![[Pasted image 20250207143231.png]]


이로써 첫 번째 항과 두 번째 항의 전개를 모두 마쳤다. 다시 본론으로 돌아가서, 이걸 왜 전개했냐? 상태 가치 함수를 나타내기 위해서이다. 전개한 식들을 다시 상태 가치 함수에 넣어보도록 하겠다. 

![[Pasted image 20250207143437.png]]

식을 보면 수익 (G) 로 표현되던 상태 가치 함수 식에서 수익 (G)가 없어졌음을 알 수 있다. 즉, 상태 가치 값을 구할 때 무한의 개념을 쓸 필요가 없다는 것이다. 이로써 우리는 상태 가치 함수를 구할 수 있게 되었다. 

### 아직 그렇게 단정 짓기는 어렵지 않아?

변형된 식을 보면 s 에서의 상태 가치 함수가 다시 s' 의 상태 가치 함수로 표현된다는 것을 알 수 있다. 이 식을 사용하면 왜 상태 가치 값을 구할 수 있는가 하면 바로 ==연립 방정식==으로 표현된다는 점이다.

이걸 구하는 방식은 교재 90P에 예시를 보면 쉽게 이해가 될 것이다. 

### 행동 가치 함수 

행동 가치 함수는 특정 행동을 했을 때의 가치를 판단하는 척도이다. 
상태 가치 함수에서 정책이 아닌 행동 (a)가 미리 결정되어서 주어진다는 점에서 크게 어렵지 않다. 
![[Pasted image 20250207144004.png]]

이를 백업 다이어그램으로 나타내면 다음과 같다. 
![[Pasted image 20250207144023.png]]

상태 가치 함수는 정책 π 를 평가하는 것이라, π 가 선택할 수 있는 모든 행동 a에 대해서 모두 시뮬레이션을 해보고 기댓값을 구하는 것인데, 행동 가치 함수는 행동 a를 평가하는 것이라, 정책이 아니라 a가 정해진다. 즉 a 행동에 대한 보상의 기댓값만 구하면 된다.


### 벨만 최적 방정식

위에서 계속 특정 정책을 판단하는 척도, 특정 행동을 판단하는 척도인 각각 상태 가치 함수, 행동 가치 함수를 배웠고, 이를 쉽게 구하기 위해 벨만 방정식을 사용했다. 

여기서 우리는 어떻게 최적의 정책을 찾을 수 있을까? 도출할 수 있는 모든 정책을 다 뽑아서, 각 정책에 대해 상태 가치 함수를 돌리고 가장 좋은 녀석을 찾으면 되나? 

일단 도출할 수 있는 모든 정책을 다 찾는다는 것이 좀 무리일 수 있다. 그래서 최적 정책을 찾는 것을 고민하는 것이 ==벨만 최적 방정식==이다. 

### 결론부터 이야기 하면 

![[Pasted image 20250207144622.png]]

이게 기존의 상태 가치 함수라면 여기서 모든 행동을 다 해보는 것이 아니라 

![[Pasted image 20250207144650.png]]

보상이 max 일 수 있는 행동만 하겠다는 것이다. 
이를 행동 가치 함수에도 적용할 수 있다. 

즉, 행동 가치 함수가 최대가 되도록 하는 행동 a 를 선택하는 정책을 고르면 되는 것이다. 
![[Pasted image 20250207144828.png]]
![[Pasted image 20250207144836.png]]

argmax 기호는 뒤의 값이 최대가 되도록 하면 인자 a 를 도출하는 식이라고 보면 된다. 

예시는 교재 103P 를 참고하자. 


### 스터디 후 잡담하다가 알게된 재밌는 사실

Greedy 알고리즘이 어이가 없다는 정근님의 말이 이어져서 네비게이션의 길찾기 기능의 동작 원리를 조금 뜯어 보았다. 

Greedy 알고리즘이 좀 어이가 없다는 것이 왜 그러냐 하면
그리디 알고리즘이 당장 내가 선택할 수 있는 최적의 방법을 선택하다보면 최종적으로도 가장 최적의 방법이 된다는 알고리즘인데, 당장 내가 선택할 수 있는 최적의 방법을 안다는 것 자체가 말이 안된다는 것이다. 

그리디가 나온 이유가 ==전체 정보를 알지 못해서== 당장 내가 알 수 있는 정보만 보겠다는 것인데, 내가 알 수 있는 정보 중에서 가치를 판단해서 최적을 뽑는다는 것 자체가 전체 정보를 알고 있다는 뜻이 되기 때문이다. 즉, 내가 알 고 있는 정보의 가치를 판단하기 위해 전체 정보를 알고 있다는 것에서 모순이 있다는 것이다. 
(근데 이게 모순이 안되는 경우도 있다. 예를 들어 사탕을 당장 많이 얻는 방향으로 선택하는 경우가 해당한다. 아마 정근님은 길찾기와 같은 navigation 분야에서 한정해서 말한 의미인 듯 하다.)

암튼 이어서 우리나라 T-map, 카카오 map 등과 같은 지도 서비스의 최적의 길찾기 안내의 원리를 좀 설명해주었는데 신기했다. 

난 모든 것을 sw 알고리즘 적으로 해결하는 줄 알았는데 그게 아니라 도로의 cctv나 구간 마다 도로의 상황을 측정할 수 있는 장치를 두고 몇 분 간격으로 상황을 판단해서 도로의 가중치를 서버에 업로드 해둔다. 그리고 지도 서비스가 일정 주기로 해당 서버로부터 도로의 실시간 정보를 받아와서 A* 알고리즘을 돌리든, Greedy를 돌리든 해서 최적의 경로를 판단한다는 것이다. 

좀만 생각하면 당연한 건데 신기했다. 알 수 없던 서비스의 로직이 이해가 되는 느낌이었다. 