
25.1.21
~57p

## Chapter 1 밴디트 문제 
---
머신 러닝(=기계 학습)은 학부 수업 '기계 학습'에서 배웠 듯이 데이터를 통해 기계가 학습하면서 입력마다 다른 출력을 낼 수도 있게 하는 학습법이다. 학습을 어떻게 시키느냐에 따라 크게 3가지로 분류한다. 

1. 지도 학습
2. 비지도 학습
3. 강화학습 

>[!note] 학부생 때 배웠던 기계학습 수업에서는 강화학습 대신 반지도 학습이라는 단어로 배웠는데 다른건가?

강화학습을 제외한 1,2번은 이미 기계학습 강의에서 들었던 개념이라 차이점만 간략하게 정리하면, 지도학습은 입력 데이터 셋에 정답 label이 있는 것을 바탕으로 학습하는 방법이고, 비지도 학습은 입력 데이터 셋에 정답 label이 없는 것을 바탕으로 학습하는 방법이다. 


### 강화학습이란?

한 문장으로 표현하면 ==에이전트가 환경과 상호작용하면서 수집한 데이터를 바탕으로 더 많은 보상을 얻는 방법을 학습하는 것== 이다. 

![[Pasted image 20250121143707.png]]

에이전트가 환경의 상태를 바탕으로 특정 행동을 하면 환경으로부터 보상과 상태를 받는다. 여기서 보상이라는 단어를 오해하면 안되는게, 보상이라는 느낌보다는 ==행동의 결과==라고 이해하면 좋을 것 같다. 즉 에이전트는 본인만의 알고리즘에 의해 상태를 바탕으로 특정 행동을 하고 얻은 보상을 최대화 하는 방향으로 학습을 한다. 

이것이 강화학습의 개념이다. 


### 강화학습에서 다루는 문제 1 : 밴디트 문제

밴디트 문제는 여러 환경이 주어지고 에이전트가 어떤 환경을 선택할지에 따라 달라지는 보상을 기반으로 다음 환경을 선택하는 것이다. 여기서 핵심은 가장 보상이 좋아지는 환경을 선택해야 한다는 것이다. 이 문제를 어떻게 풀 수 있을까?

>[!info] 밴디트는 슬롯머신의 이름인데, 여러 슬롯 머신 중 가장 당첨 확률이 높은 머신을 선택한다는 느낌과 같다고 판단해서 밴디트 문제라고 한다. 

대표적인 예시를 들어보면 도박장에서 여러 개의 슬롯 머신이 존재하고, 각 슬롯에서 얻을 수 있는 코인은 0,1,5,10 이 있는데 각각 나올 수 있는 확률이 다르게 존재한다. 이때 플레이어는 1000번의 기회가 있고 얻을 수 있는 가장 많은 코인을 얻고 싶다. 

그러면 여러 개의 슬롯 머신이 존재할 때 각각 어떤게 더 좋은 것인지 판단하는 척도가 필요하다. 여기서 사용하는 것이 확률의 기댓값이다. 만약 어떤 슬롯 머신들이 존재하고, 각 슬롯 머신의 확률분포표 (슬롯을 실행했을 때 특정 값이 나올 확률들의 표)를 미리 알고 있다면, 

```
(나올 값) * (해당 값의 확률)   --> 이걸 모든 값에 대해 계산하고 더한 값이 해당 슬롯의 전체 기댓값이다.  
```

수식으로 표현하면 다음과 같다. 
- Q() 는 행동가치이다. 괄호 안에 들어가는 행동을 취했을 때 얻는 가치를 나타낸다. 
- E 는 기댓값이다.
- R 은 확률 변수다. 특정 행동을 했을 때 얻을 수 있는 상태 집합이다. 
- A 는 행동이다. 에이전트가 취할 수 있는 행동 집합이다. 

```
Q(A) = E[R|A]
```


### 그런데

위의 예시와 같이 각 슬롯머신의 확률 분포표를 전부 알고 있다면 각 슬롯을 선택하는 행동에 대한 Q를 계산할 수 있으므로 가장 큰 Q를 가지는 슬롯을 계속 실행하면 best가 되는데 보통의 경우에는 각 슬롯 머신의 확률분포표를 알 수가 없다. 

그래서 사용하는 방법이 ==가치 추정 방법==이다. 실행해보면서 각 슬롯의 가치를 추정해보고 다음 슬롯을 결정한다는 방법이다. 

![[Pasted image 20250121155411.png]]

각 슬롯을 3번까지 각각 실행해봤다고 하면 결과값을 얻을 수 있다. 여기서 각 슬롯의 가치를 어떻게 계산하냐 하면 똑같이 기댓값을 사용하면 된다. 

슬롯 a의 가치 : (0+1+5) / 3 = 2
슬롯 b의 가치 : (1+0+0) / 3 = 0.3333...

이렇게 3번째까지 실행했을 때 A의 가치가 더 높은 것을 확인할 수 있고, 다음번 플레이어는 a 머신을 선택할 것이다. 


### 지금까지 로직을 최적화 해보자. 

각 회차마다 기댓값을 구할 때, 다음과 같은 수식을 쓰면 문제가 뭘까?

![[Pasted image 20250121155934.png]]

맞다. n-1 회차에서 계산 후에 n 회차에서 계산할 때 이전 회차에서 계산했던 R1 + R2 + R3 + ... + R(n-1) 까지의 과정을 다시 계산한다는 점이다. 이전 Q(n-1) 값에 새로 나온 값을 더해주는 형식이면 좋을 것 같다.

식을 변환해보자. 

![[Pasted image 20250121160109.png]]
![[Pasted image 20250121160134.png]]
![[Pasted image 20250121160145.png]]

즉 최종 식은 위와 같은 형태가 된다.
많이 본 형태 아닌가? 기계학습에서 Gradient Decent 알고리즘에서 W를 학습할 때와 비슷하다. 그렇게 보면 1 / n 값이 학습률이라고 볼 수 있다. (이 개념은 뒤에서 쓰인다. )

>[! tip] 시도 횟수 n이 커질수록 1/n 은 작아진다. 즉 시도 횟수가 늘어날수록 Q(n)이 갱신되는 양이 작아진다는 뜻이다. 이렇게 시도 횟수가 늘어날수록 기댓값이 특정 값에 수렴하는 것을 보면 중학교 수학 시간에 배웠던 '큰 수의 법칙' 이 떠오른다. 



### 시도횟수마다 각 환경의 행동가치가 갱신되니까 다음 시도에서는 가장 큰거 선택하면 되나?

이렇게 선택하는 방식을 Greedy 방식이라고 한다. (ps 분류) 

하지만 이 방식의 문제가 있다. 당장에 안좋은 선택을 하는 것이 장기적으로 보았을 때 더 좋은 경우도 있기 때문이다. 이건 책의 구체적인 예시를 보면 좋다. 
원인은 슬롯 머신의 가치 추정치에 '불확실성'이 있기 때문에 생기는 문제이다. ==확실하지 않은 추정치를 전적으로 신뢰하면 최선의 행동을 놓칠 수 있다. ==

그럼 각 시도마다 플레이어가 선택할 수 있는 선택지는 2개이다. 

1. 활용 : 지금까지 실제로 플레이한 결과를 바탕으로 가장 좋다고 생각되는 슬롯머신을 플레이 
2. 탐색 : 슬롯머신의 가치를 정확하게 추정하기 위해 다양한 슬롯 머신을 시도 

즉, 앞서 말한 Greedy 방식은 매 시도마다 활용 방법을 사용한 것이다. 

여기서 강화학습 알고리즘은 결국 '활용과 탐색의 균형' 을 얼마나 잡을 것인가 가 관건이다. 이걸 잘하기 위해서 쓰는 대표적인 알고리즘이 ==ε-Greedy== (입실론-탐욕정책)이다. 이건 간단한데, ε 이 0.1이면 매 시도에서 10% 확률도 탐색하고, 90% 확률로 활용한다는 정책이다. 


### 비정상 문제

앞서 다룬 문제는 ==정상문제==이다. 정상문제란 보상의 확률이 매 시도마다 변하지 않는 문제를 말한다. 하지만 비정상 문제는 플레이어가 시도를 할 때마다 각 환경의 각 보상의 확률이 변한다는 문제이다. 이때는 어떻게 문제를 풀 수 있을까? 

![[Pasted image 20250121160145.png]]

아이디어는 다음과 같다. 
위 식에서 쓰이는 가중치( =학습률)는 모든 보상에 똑같은 가중치가 부여된다는 뜻이다. 무슨 말이냐면 새로 얻은 보상이든 오래전에 얻은 보상이든 모두 동등한 비율로 새로 갱신될 행동가치에 반영하겠다는  뜻이다. 

그럼 비정상 문제에서는 가치가 시도마다 바뀌니까 행동가치를 갱신할 때 새로 얻은 보상의 반영 비율을 높게 잡고 오래전에 얻은 가치일수록 적게 잡는 것이 바람직하다. 
이렇게 하기 위한 방법은 가중치를 1/n으로 두는 것이 아니라 고정값인 α (알파) 로 두는 것이다. (0 < α < 1 )

![[Pasted image 20250121162446.png]]

이렇게 하면 왜 오래 전에 얻은 보상은 적게 반영된다는 걸까? 간략하게 말하면 1보다 작은 α가 계속 곱해지니까 해당 보상의 값이 엄청 작아지게 된다. (자세한 수식으로 확인하려면 54p를 보면 된다.) 이 방식을 사용한 것을 '지수 이동 평균' 이라고 한다.


### 질문

- 밴디트 문제의 구체적인 예시가 있는지? 책에서는 슬롯머신으로만 다루니까 감이 잘 안 잡힘.


### 스터디

